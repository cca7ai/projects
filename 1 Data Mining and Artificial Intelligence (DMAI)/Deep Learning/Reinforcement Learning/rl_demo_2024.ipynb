{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[toy-text] in /home/autoj/ml_env_v1/lib/python3.10/site-packages (0.29.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/autoj/ml_env_v1/lib/python3.10/site-packages (from gymnasium[toy-text]) (0.0.4)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/autoj/ml_env_v1/lib/python3.10/site-packages (from gymnasium[toy-text]) (1.26.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/autoj/ml_env_v1/lib/python3.10/site-packages (from gymnasium[toy-text]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/autoj/ml_env_v1/lib/python3.10/site-packages (from gymnasium[toy-text]) (4.9.0)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /home/autoj/ml_env_v1/lib/python3.10/site-packages (from gymnasium[toy-text]) (2.5.2)\n",
      "Requirement already satisfied: numpy in /home/autoj/ml_env_v1/lib/python3.10/site-packages (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "# Executer cette cellule si hymnasium et numpy ne sont pas installés\n",
    "!pip install gymnasium[toy-text]\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Xeg-g8JaAzrf"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gdXDomxsMiX"
   },
   "source": [
    "### <font color=\"blue\"> Un environnement de jouet !\n",
    "Vous pouvez produire une rendue de l'environnement. Il exist plusieurs modes de rendu:\n",
    "-  \"human\": Affiche l'environnement dans une fenêtre à l'aide du backend de rendu. Il n'est pas recommander d'utiliser cette option durant l'entrainement car cela ralentie considérablement le processus d'entrainement.\n",
    "- \"rgb_array\": Renvoie un tableau RGB représentant l'image rendue sans l'afficher dans une fenêtre. Util si vous souhaitez enregistrer une video du processus d'entrainement sans avoir un impact sur la vitesse du processus d'entrainement.\n",
    "- \"ansi\": Rend l'environnement sous forme de texte dans la console en utilisant des codes d'échappement ANSI.\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWl3auFRBBHr",
    "outputId": "64412257-2867-474b-fb3d-e61312cfec20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace des états:  Discrete(16)\n",
      "Espace des actions:  Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=\"human\") # Environnement avec des actions deterministics avec l'argument is_slippery=False\n",
    "env.reset()\n",
    "\n",
    "# L'espace des états et l'espace des actions.\n",
    "print(\"Espace des états: \", env.observation_space)\n",
    "print(\"Espace des actions: \", env.action_space)\n",
    "\n",
    "env.render()\n",
    "time.sleep(10.0)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jmecHQ4DlZq"
   },
   "source": [
    "### <font color=\"blue\"> Nous allons interagir avec l'environnement en choisissant les actions de manière aléatoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IM-3-88nBq-W"
   },
   "outputs": [],
   "source": [
    "def interacte_with_env(env, iter=100):\n",
    "    env.reset()\n",
    "    for _ in range(iter):\n",
    "      env.render()\n",
    "      time.sleep(0.5)\n",
    "      action = env.action_space.sample() #Choisir une action de manière aléatoire\n",
    "      observation, reward, terminated, truncated, info = env.step(action)\n",
    "      #print(\"Récompense: \", reward)\n",
    "      #print(\"Info: \", info)\n",
    "      if terminated or truncated: #verifier si l'état (observation) est un état terminal\n",
    "        observation = env.reset()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appeler la fonction d'interaction pour interagir de manière aléatoire avec l'environnement\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=\"human\")\n",
    "interacte_with_env(env, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gacGmjWPGDFR"
   },
   "source": [
    "### <font color=\"blue\"> Carte customisé\n",
    " Cette environnement simple que nous utilisons est une carte avec des lac. Vous pouvez customiser ce environnement en taille, emplacement et nombre des lac et emplacement de l'etat final qui a une recompense\n",
    " - S: état initial\n",
    " - F: état normal\n",
    " - H: lac\n",
    " - G: état final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-4C6zaQAGJJX"
   },
   "outputs": [],
   "source": [
    "default_frozen_lake = [\n",
    "    \"SFFF\",\n",
    "    \"FHFH\",\n",
    "    \"FFFH\",\n",
    "    \"HFFG\"\n",
    "    ]\n",
    "\n",
    "custom_frozen_lake=[\n",
    "                    'SFHFHF',\n",
    "                    'FFHFFF',\n",
    "                    'FHHHFF',\n",
    "                    'FFFHHH',\n",
    "                    'FHFFHF',\n",
    "                    'FHFHFG',\n",
    "                    'HFFFFH'\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "eo9hkzLvwMsi"
   },
   "outputs": [],
   "source": [
    "# Créer l'environnement avec notre carte customizé et appeler la fonction d'interaction definie plus haut\n",
    "env = gym.make('FrozenLake-v1', desc=custom_frozen_lake, render_mode=\"human\")\n",
    "interacte_with_env(env, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous definisons une fonction utilitaire pour executer une politique donnée.\n",
    "# Elle sera utiliser après\n",
    "def execution_politique(politique, env):\n",
    "    state, _ = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not (terminated or truncated):\n",
    "        env.render()\n",
    "        time.sleep(0.5)\n",
    "        action = int(politique[state])\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "    time.sleep(2.0)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q3FYSs_1KoN"
   },
   "source": [
    "### <font color='blue'/> Evaluation d'une politique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mgSe87Nq1JgT"
   },
   "outputs": [],
   "source": [
    "# Une fonction pour evaluer une politique donnée.\n",
    "# Rappel: L'evaluation de la politique corresponds au calcul de la fonction de valeur associé à cette politique\n",
    "def evaluation_politique(env, politique, gamma=0.9):\n",
    "  Vs = np.zeros(env.observation_space.n)\n",
    "  theta = 1e-20\n",
    "  num_iters = 0\n",
    "  while True:\n",
    "    num_iters += 1 # cette variable ne fait pas partie de l'algo, c'est juste pour voir à combien d'itération on va converger.\n",
    "    delta = 0\n",
    "    for state in range(env.observation_space.n):\n",
    "          v = Vs[state]\n",
    "          action = politique[state]\n",
    "          r_plus_Vsprime = 0\n",
    "          for sprime_r in env.unwrapped.P[state][action]:\n",
    "              p_sprime_r, sprime, recompense,_ = sprime_r\n",
    "              r_plus_Vsprime += p_sprime_r * (recompense + gamma*Vs[sprime])\n",
    "          Vs[state] = r_plus_Vsprime\n",
    "          diff_val = np.fabs(v - Vs[state])\n",
    "          delta = max(delta, diff_val)\n",
    "    print(\"iteration: \",num_iters, \"  diff: \", delta)\n",
    "    if delta <= theta:\n",
    "          print(\"Convergence de Vs après \", num_iters, \" itérations !!!\")\n",
    "          print(\"----------------------------\")\n",
    "          break\n",
    "  return Vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hRMZdg521cKf",
    "outputId": "66ee9c93-3be0-424d-f3ec-53a531aef31d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1   diff:  1.0\n",
      "iteration:  2   diff:  0.9\n",
      "iteration:  3   diff:  0.81\n",
      "iteration:  4   diff:  0.7290000000000001\n",
      "iteration:  5   diff:  0.6561000000000001\n",
      "iteration:  6   diff:  0.5904900000000002\n",
      "iteration:  7   diff:  0.5314410000000002\n",
      "iteration:  8   diff:  0.47829690000000014\n",
      "iteration:  9   diff:  0.43046721000000016\n",
      "iteration:  10   diff:  0.38742048900000015\n",
      "iteration:  11   diff:  0\n",
      "Convergence de Vs après  11  itérations !!!\n",
      "----------------------------\n",
      "[0.38742049 0.34867844 0.         0.         0.         0.\n",
      " 0.43046721 0.38742049 0.         0.         0.         0.\n",
      " 0.4782969  0.         0.         0.         0.         0.\n",
      " 0.531441   0.59049    0.6561     0.         0.         0.\n",
      " 0.4782969  0.         0.729      0.81       0.         1.\n",
      " 0.43046721 0.         0.81       0.9        1.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# ici nous definisons une politique pi_1 et nous lévaluons\n",
    "env = gym.make('FrozenLake-v1', desc=custom_frozen_lake, is_slippery=False, render_mode=\"human\")\n",
    "pi_1 = [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 3, 0, 1, 1, 0, 1, 3, 0, 2, 2, 2, 0]\n",
    "Vs_evalue = evaluation_politique(env, pi_1, gamma=0.9)\n",
    "print(Vs_evalue) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=custom_frozen_lake, is_slippery=False, render_mode=\"human\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1   diff:  1.0\n",
      "iteration:  2   diff:  0.92\n",
      "iteration:  3   diff:  0.8464\n",
      "iteration:  4   diff:  0.778688\n",
      "iteration:  5   diff:  0.7163929600000001\n",
      "iteration:  6   diff:  0\n",
      "Convergence de Vs après  6  itérations !!!\n",
      "----------------------------\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.71639296 0.         0.         0.\n",
      " 0.         0.         0.778688   0.8464     0.         1.\n",
      " 0.         0.         0.8464     0.92       1.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# ici nous definisons une politique pi_2 et nous lévaluons (OBSERVER COMMENT LES VALEURS obtenues avec pi_2 different avec celles obtenues \n",
    "# avec pi_2. En effet la politique pi_1 est une politique optimale que j'ai expressement choisie. Pour chacun des 36 état, pi_1 a des V(s)\n",
    "# meilleures que pi_2)\n",
    "env = gym.make('FrozenLake-v1', desc=custom_frozen_lake, is_slippery=False, render_mode=\"human\")\n",
    "pi_2 = [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 3, 1, 0, 0, 0, 3, 0, 1, 1, 0, 1, 3, 0, 2, 2, 2, 0]\n",
    "Vs_evalue = evaluation_politique(env, pi_2, gamma=0.92)\n",
    "print(Vs_evalue) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vous pouvez executer une des politique ici pour voir ce que ca donne.\n",
    "env = gym.make('FrozenLake-v1', desc=custom_frozen_lake, is_slippery=False, render_mode=\"human\")\n",
    "execution_politique(pi_1, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQbyVfOqcppF"
   },
   "source": [
    "### <font color=\"blue\"> Utilisons quelques algorithmes d'apprentissage par renforcement pour que l'agent puisse apprendre à éviter les troues et atteindre l'état final G."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OdeN57pH3ps"
   },
   "source": [
    "Nous utilisons la **méthode d'itération de valeur**.\n",
    "\n",
    "Rappel:\n",
    "1. L'itération de valeur est une méthode de programmation dynamique\n",
    "2. En entrée nous avons besoin du modèle de l'environnement (Fonction de transition et de récompense), les espaces des états et actions.\n",
    "3. La méthode étant itérative, nous avons besoin de définir la condition d'arrêt. Nous allons indiquez un nombre maximum d'itérations à ne pas dépasser et un paramètre delta de convergence.\n",
    "4. En sortie nous aurons une fonction de valeur optimale et une politique optimale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kJnVMr4b8fn"
   },
   "source": [
    "Nous utilisons la **méthode d'itération de politique.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cSHEToXMSZkV"
   },
   "outputs": [],
   "source": [
    "# Nous devons extraire la politique optimale à partir de fonction de valeur obtenue\n",
    "# Cette fonction est utiliser dans l'itération de politique et l'itération de valeur.\n",
    "def extraire_politique(env, Vs, gamma=0.9):\n",
    "  # Definir la politique\n",
    "  politique = np.zeros(env.observation_space.n)    \n",
    "  for state in range(env.observation_space.n):\n",
    "    Qs = np.zeros(env.action_space.n)\n",
    "    for action in range(env.action_space.n):\n",
    "      for sprime_r in env.unwrapped.P[state][action]:\n",
    "          p_sprime_r, sprime, recompense,_ = sprime_r\n",
    "          Qs[action] += p_sprime_r * (recompense + gamma*Vs[sprime])\n",
    "    politique[state] = np.argmax(Qs)\n",
    "  return politique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sampled_value_integer = np.random.randint(0, 4)\n",
    "\n",
    "print(sampled_value_integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "y0AN0F2Pdi1G"
   },
   "outputs": [],
   "source": [
    "# La fonction pour l'iteration de politique\n",
    "def iteration_politique(env,  max_iter=5000, gamma=0.9):\n",
    "  # Definir une politique initiale\n",
    "  politique = np.zeros(env.observation_space.n) # une politique initial\n",
    "  for i in range(env.observation_space.n):\n",
    "      politique[i] = np.random.randint(0, 4)\n",
    "\n",
    "  # Faire l'iteration de politique (succession entre evaluation et amélioration)\n",
    "  i = 0\n",
    "  while True:\n",
    "    Vs = evaluation_politique(env, politique, gamma) # evaluation de la politique\n",
    "    nouvelle_politique = extraire_politique(env, Vs, gamma)   # amélioration de la politique\n",
    "    # Verifier si la politique est stable\n",
    "    if (np.all(nouvelle_politique == politique)):\n",
    "      print(\"La politique a convergé après \", i, \" itérations !\")\n",
    "      break\n",
    "    # Si la politique n'est pas stable alors continuer\n",
    "    politique = nouvelle_politique\n",
    "    i += 1\n",
    "  return nouvelle_politique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ns-GVg6sxrEb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1   diff:  0\n",
      "Convergence de Vs après  1  itérations !!!\n",
      "----------------------------\n",
      "iteration:  1   diff:  1.0\n",
      "iteration:  2   diff:  0\n",
      "Convergence de Vs après  2  itérations !!!\n",
      "----------------------------\n",
      "iteration:  1   diff:  1.0\n",
      "iteration:  2   diff:  0\n",
      "Convergence de Vs après  2  itérations !!!\n",
      "----------------------------\n",
      "iteration:  1   diff:  1.0\n",
      "iteration:  2   diff:  0.81\n",
      "iteration:  3   diff:  0\n",
      "Convergence de Vs après  3  itérations !!!\n",
      "----------------------------\n",
      "iteration:  1   diff:  1.0\n",
      "iteration:  2   diff:  0.81\n",
      "iteration:  3   diff:  0.7290000000000001\n",
      "iteration:  4   diff:  0\n",
      "Convergence de Vs après  4  itérations !!!\n",
      "----------------------------\n",
      "iteration:  1   diff:  1.0\n",
      "iteration:  2   diff:  0.81\n",
      "iteration:  3   diff:  0.7290000000000001\n",
      "iteration:  4   diff:  0.6561000000000001\n",
      "iteration:  5   diff:  0\n",
      "Convergence de Vs après  5  itérations !!!\n",
      "----------------------------\n",
      "iteration:  1   diff:  1.0\n",
      "iteration:  2   diff:  0.81\n",
      "iteration:  3   diff:  0.7290000000000001\n",
      "iteration:  4   diff:  0.6561000000000001\n",
      "iteration:  5   diff:  0.5904900000000002\n",
      "iteration:  6   diff:  0\n",
      "Convergence de Vs après  6  itérations !!!\n",
      "----------------------------\n",
      "iteration:  1   diff:  1.0\n",
      "iteration:  2   diff:  0.81\n",
      "iteration:  3   diff:  0.7290000000000001\n",
      "iteration:  4   diff:  0.6561000000000001\n",
      "iteration:  5   diff:  0.5904900000000002\n",
      "iteration:  6   diff:  0.5314410000000002\n",
      "iteration:  7   diff:  0\n",
      "Convergence de Vs après  7  itérations !!!\n",
      "----------------------------\n",
      "iteration:  1   diff:  1.0\n",
      "iteration:  2   diff:  0.81\n",
      "iteration:  3   diff:  0.7290000000000001\n",
      "iteration:  4   diff:  0.6561000000000001\n",
      "iteration:  5   diff:  0.5904900000000002\n",
      "iteration:  6   diff:  0.5314410000000002\n",
      "iteration:  7   diff:  0.47829690000000014\n",
      "iteration:  8   diff:  0\n",
      "Convergence de Vs après  8  itérations !!!\n",
      "----------------------------\n",
      "iteration:  1   diff:  1.0\n",
      "iteration:  2   diff:  0.81\n",
      "iteration:  3   diff:  0.7290000000000001\n",
      "iteration:  4   diff:  0.6561000000000001\n",
      "iteration:  5   diff:  0.5904900000000002\n",
      "iteration:  6   diff:  0.5314410000000002\n",
      "iteration:  7   diff:  0.47829690000000014\n",
      "iteration:  8   diff:  0.43046721000000016\n",
      "iteration:  9   diff:  0\n",
      "Convergence de Vs après  9  itérations !!!\n",
      "----------------------------\n",
      "iteration:  1   diff:  1.0\n",
      "iteration:  2   diff:  0.81\n",
      "iteration:  3   diff:  0.7290000000000001\n",
      "iteration:  4   diff:  0.6561000000000001\n",
      "iteration:  5   diff:  0.5904900000000002\n",
      "iteration:  6   diff:  0.5314410000000002\n",
      "iteration:  7   diff:  0.47829690000000014\n",
      "iteration:  8   diff:  0.43046721000000016\n",
      "iteration:  9   diff:  0.38742048900000015\n",
      "iteration:  10   diff:  0\n",
      "Convergence de Vs après  10  itérations !!!\n",
      "----------------------------\n",
      "iteration:  1   diff:  1.0\n",
      "iteration:  2   diff:  0.81\n",
      "iteration:  3   diff:  0.7290000000000001\n",
      "iteration:  4   diff:  0.6561000000000001\n",
      "iteration:  5   diff:  0.5904900000000002\n",
      "iteration:  6   diff:  0.5314410000000002\n",
      "iteration:  7   diff:  0.47829690000000014\n",
      "iteration:  8   diff:  0.43046721000000016\n",
      "iteration:  9   diff:  0.38742048900000015\n",
      "iteration:  10   diff:  0.34867844010000015\n",
      "iteration:  11   diff:  0\n",
      "Convergence de Vs après  11  itérations !!!\n",
      "----------------------------\n",
      "iteration:  1   diff:  1.0\n",
      "iteration:  2   diff:  0.81\n",
      "iteration:  3   diff:  0.7290000000000001\n",
      "iteration:  4   diff:  0.6561000000000001\n",
      "iteration:  5   diff:  0.5904900000000002\n",
      "iteration:  6   diff:  0.5314410000000002\n",
      "iteration:  7   diff:  0.47829690000000014\n",
      "iteration:  8   diff:  0.43046721000000016\n",
      "iteration:  9   diff:  0.38742048900000015\n",
      "iteration:  10   diff:  0.34867844010000015\n",
      "iteration:  11   diff:  0.31381059609000017\n",
      "iteration:  12   diff:  0\n",
      "Convergence de Vs après  12  itérations !!!\n",
      "----------------------------\n",
      "iteration:  1   diff:  1.0\n",
      "iteration:  2   diff:  0.81\n",
      "iteration:  3   diff:  0.7290000000000001\n",
      "iteration:  4   diff:  0.6561000000000001\n",
      "iteration:  5   diff:  0.5904900000000002\n",
      "iteration:  6   diff:  0.5314410000000002\n",
      "iteration:  7   diff:  0.47829690000000014\n",
      "iteration:  8   diff:  0.43046721000000016\n",
      "iteration:  9   diff:  0.38742048900000015\n",
      "iteration:  10   diff:  0.34867844010000015\n",
      "iteration:  11   diff:  0.31381059609000017\n",
      "iteration:  12   diff:  0\n",
      "Convergence de Vs après  12  itérations !!!\n",
      "----------------------------\n",
      "La politique a convergé après  13  itérations !\n",
      "[1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 2. 2. 1. 0. 0. 0.\n",
      " 3. 0. 1. 0. 0. 1. 3. 0. 1. 0. 2. 0. 0. 2. 2. 2. 3. 0.]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=custom_frozen_lake, is_slippery=False, render_mode=\"human\")\n",
    "politique_op_it = iteration_politique(env, 5000, 0.9)\n",
    "print(politique_op_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Luhk7FtowMsk"
   },
   "outputs": [],
   "source": [
    "# Appeler la fonction pour executer la politique optimale obtenue plus haut.\n",
    "env = gym.make('FrozenLake-v1', desc=custom_frozen_lake, is_slippery=False, render_mode=\"human\")\n",
    "execution_politique(politique_op_it, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oww_RROVIaTo"
   },
   "outputs": [],
   "source": [
    "# Une fonction permettant de faire l'itération de valeur\n",
    "def iteration_valeur(env, max_iter=5000, gamma=0.9): #lambda est le coéfficient de dépreciation\n",
    "  Vs = np.zeros(env.observation_space.n) # Initialisation de la fonction de valeur pour tous les états.\n",
    "  epsilon = 1e-5 #Pour verifier la convergence\n",
    "  num_iters = 0\n",
    "  for i in range(max_iter):\n",
    "    delta = 0\n",
    "    num_iters += 1 # cette variable ne fait pas partie de l'algo, c'est juste pour voir à combien d'itération on va converger.\n",
    "    #Vs_copy = np.copy(Vs) #garder une copie de la fonction des valeurs pour comparaison\n",
    "    for state in range(env.observation_space.n):\n",
    "      Qs = []\n",
    "      v = Vs[state]\n",
    "      for action in range(env.action_space.n):\n",
    "        r_plus_Vsprime = []\n",
    "        for sprime_r in env.unwrapped.P[state][action]:\n",
    "          p_sprime_r, sprime, recompense,_ = sprime_r\n",
    "          r_plus_Vsprime.append(p_sprime_r * (recompense + gamma*Vs[sprime]))\n",
    "        Qs.append(np.sum(r_plus_Vsprime))\n",
    "      Vs[state] = max(Qs)\n",
    "      delta = max(delta, np.fabs(v - Vs[state]))\n",
    "    if delta <= epsilon:\n",
    "      break\n",
    "  print(\"Convergence après \", num_iters, \" itérations !!!\")\n",
    "  return Vs, Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0gYESZcwVa5Z",
    "outputId": "532ec049-7b6e-4cf2-84ac-8dba52f908b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence après  12  itérations !!!\n",
      "[0.3138106  0.28242954 0.         0.         0.         0.\n",
      " 0.34867844 0.3138106  0.         0.         0.         0.\n",
      " 0.38742049 0.         0.         0.         0.         0.\n",
      " 0.43046721 0.4782969  0.531441   0.         0.         0.\n",
      " 0.38742049 0.         0.59049    0.531441   0.         1.\n",
      " 0.34867844 0.         0.6561     0.         1.         0.\n",
      " 0.         0.6561     0.729      0.81       0.9        0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Utiliser la fonction d'iteration de valeur pour trouver une politique optimale\n",
    "gamma = 0.9\n",
    "env = gym.make('FrozenLake-v1', desc=custom_frozen_lake, is_slippery=False, render_mode=\"rgb_array\")\n",
    "Vs, Q = iteration_valeur(env, max_iter=5000, gamma=gamma)\n",
    "print(Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KnNuaCpcXlhP",
    "outputId": "d3f04114-2150-41b8-999e-98cb7d2fb0c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 2. 2. 1. 0. 0. 0.\n",
      " 3. 0. 1. 0. 0. 1. 3. 0. 1. 0. 2. 0. 0. 2. 2. 2. 3. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Utiliser la fonction d'extraction de politique pour extraire la politique optimale\n",
    "env = gym.make('FrozenLake-v1', desc=custom_frozen_lake, is_slippery=False, render_mode=\"human\")\n",
    "politique_op = extraire_politique(env, Vs, gamma=gamma)\n",
    "print(politique_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_YhFs_aLwMsl"
   },
   "outputs": [],
   "source": [
    "# Executon la politique optimale\n",
    "# Appeler la fonction pour executer la politique\n",
    "env = gym.make('FrozenLake-v1', desc=custom_frozen_lake, is_slippery=False, render_mode=\"human\")\n",
    "execution_politique(politique_op, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> Méthode de Monte Carlo pour le contrôle (ou la recherche de politique optimale) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "dlFyJicpZVlH"
   },
   "outputs": [],
   "source": [
    "# Politique epsilon-gloutonne\n",
    "# epsilon est typiquement très faible\n",
    "def epsilon_gloutonne(env, state, Qs, epsilon):\n",
    "  if np.random.rand() < epsilon:\n",
    "    action = env.action_space.sample()\n",
    "  else:\n",
    "    action = max(list(range(env.action_space.n)), key = lambda x: Qs[(state,x)])\n",
    "  return action\n",
    "\n",
    "# Politique final Gloutonne\n",
    "def extraire_politique_Qs(env, Qs):\n",
    "  politique = np.zeros(env.observation_space.n)\n",
    "  for state in range(env.observation_space.n):\n",
    "    action = max(list(range(env.action_space.n)), key = lambda x: Qs[(state,x)])\n",
    "    politique[state] = action\n",
    "  return politique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "# La fonction qui implemente le contrôle MC avec la politique espilon-gloutonne\n",
    "def mc_control(env, num_episodes, episode_max_length = 100, gamma=0.9, epsilon=0.1):\n",
    "    sum_Rt = defaultdict(float)\n",
    "    num_Rt = defaultdict(float) \n",
    "    Qs = {}\n",
    "    for s in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            Qs[(s,a)] = 0.0\n",
    "    for episode in range(1, num_episodes+1):\n",
    "        #Generer l séquence état-action-récompense pour l'épisode actuel\n",
    "        episode_sar = []\n",
    "        state, _ = env.reset()\n",
    "        for t in range(episode_max_length):\n",
    "            action = epsilon_gloutonne(env, state, Qs, epsilon)\n",
    "            sprime, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_sar.append((state, action, reward))\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = sprime\n",
    "        #print(episode_sar)\n",
    "        # Trouver tous les couples état-action dans cette épisode\n",
    "        sa_in_episode = set([(x[0], x[1]) for x in episode_sar]) # set permet de retenir une seule instance de chaque couple état-action\n",
    "        # Pour chaque couple (etat-action) calculer le retour attendue à partir de sa prémière apparution\n",
    "        for sa in sa_in_episode:\n",
    "            state = sa[0]\n",
    "            action = sa[1]\n",
    "            for i, x in enumerate(episode_sar):\n",
    "                if x[0] == state and x[1]==action:\n",
    "                    first_occurence_idx = i\n",
    "                    break\n",
    "            # La somme des recompense à partir de la première apparution du couple (etat, action) dans l'épisode\n",
    "            #print(sa)\n",
    "            #print(first_occurence_idx)\n",
    "            Rt = 0.0\n",
    "            k = 0\n",
    "            for x in episode_sar[first_occurence_idx:]:\n",
    "                Rt += (x[2]*gamma)\n",
    "                k += 1\n",
    "            sum_Rt[sa] += Rt\n",
    "            num_Rt[sa] += 1.0\n",
    "            Qs[(state, action)] =  sum_Rt[sa] / num_Rt[sa]\n",
    "\n",
    "        # Imprimer le numéro de l'épisode actuelle (juste pour suivie de l'entrainement)\n",
    "        if episode%500 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "    print(\"\")\n",
    "    return Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50000/50000.\n",
      "Q(s, a): {(0, 0): 0.22977973568283067, (0, 1): 0.5141905425413006, (0, 2): 0.2613453339649575, (0, 3): 0.28805620608900623, (1, 0): 0.09275039036359516, (1, 1): 0.0, (1, 2): 0.5157246782316913, (1, 3): 0.2652303120356583, (2, 0): 0.16739606126914713, (2, 1): 0.6454819277108679, (2, 2): 0.4088642659279791, (2, 3): 0.516209476309229, (3, 0): 0.4920000000000015, (3, 1): 0.0, (3, 2): 0.3056603773584906, (3, 3): 0.2755102040816325, (4, 0): 0.24108265424913708, (4, 1): 0.6061759925704105, (4, 2): 0.0, (4, 3): 0.3464774104206475, (5, 0): 0.0, (5, 1): 0.0, (5, 2): 0.0, (5, 3): 0.0, (6, 0): 0.0, (6, 1): 0.7845991561181942, (6, 2): 0.0, (6, 3): 0.6440758293838881, (7, 0): 0.0, (7, 1): 0.0, (7, 2): 0.0, (7, 3): 0.0, (8, 0): 0.3890695915280042, (8, 1): 0.0, (8, 2): 0.6976510067116118, (8, 3): 0.4333255651363473, (9, 0): 0.49584415584417085, (9, 1): 0.7710769230769593, (9, 2): 0.7849026561483644, (9, 3): 0.0, (10, 0): 0.6807829181494869, (10, 1): 0.882019713735662, (10, 2): 0.0, (10, 3): 0.6594339622641697, (11, 0): 0.0, (11, 1): 0.0, (11, 2): 0.0, (11, 3): 0.0, (12, 0): 0.0, (12, 1): 0.0, (12, 2): 0.0, (12, 3): 0.0, (13, 0): 0.0, (13, 1): 0.7908415841584148, (13, 2): 0.8864143056201264, (13, 3): 0.6819477434679344, (14, 0): 0.802741935483903, (14, 1): 0.8814170040486228, (14, 2): 0.9000000000003463, (14, 3): 0.7842397776895903, (15, 0): 0.0, (15, 1): 0.0, (15, 2): 0.0, (15, 3): 0.0}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=default_frozen_lake, is_slippery=False, render_mode=\"rgb_array\")\n",
    "Qs = mc_control(env, num_episodes=50000, episode_max_length = 50, gamma=0.9, epsilon=0.3)\n",
    "print(f\"Q(s, a): {Qs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politique: [1. 2. 1. 0. 1. 0. 1. 0. 2. 2. 1. 0. 0. 2. 2. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Extraire la politique gloutonne à partir de la connaissance de Qs\n",
    "politique_mc =extraire_politique_Qs(env, Qs)\n",
    "print(f\"Politique: {politique_mc}\")\n",
    "# Executer la politique\n",
    "env = gym.make('FrozenLake-v1', desc=default_frozen_lake, is_slippery=False, render_mode=\"human\")\n",
    "execution_politique(politique_mc, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZ_A8FPrYr-l"
   },
   "source": [
    "### <font color=\"blue\"> Utilisons un des algorithmes d'apprentissage par renforcement pour que l'agent puisse apprendre à éviter les troues et atteindre l'état final G. Nous utilisons en premier lieu la **méthode Q-learning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "fRxEpMPY4PIC"
   },
   "outputs": [],
   "source": [
    "# Q-Learning\n",
    "def q_learning(env,Q_init, epsilon, gamma, alpha, max_episodes=5000):\n",
    "  #Initialiser la fonction action-valeur\n",
    "  Qs = Q_init\n",
    "  # for each episode\n",
    "  for i in range(max_episodes):\n",
    "    state, _ = env.reset() #initialiser l'environnement au debut de l'épisode\n",
    "    iter = 0\n",
    "    while True: # Tant que l'épisode n'est pas fini\n",
    "      # Selectionner une action suivant une politique epsilon-gloutonne\n",
    "      action = epsilon_gloutonne(env, state, Qs, epsilon)\n",
    "      #print(action)\n",
    "\n",
    "      # Entreprendre l'action selectionné et observer la récompense et le nouveau état\n",
    "      sprime, reward, terminated, truncated, info = env.step(action)\n",
    "      # Mettre la fonction action-valeur à jour\n",
    "      Q_max = max(Qs[(sprime, a)] for a in range(env.action_space.n))\n",
    "      Qs[(state, action)] += alpha*(reward + gamma*Q_max - Qs[(state, action)])\n",
    "      # remplacer l'état actuelle avec le nouveau état pour continuer à envancer dans l'environnement\n",
    "      state = sprime\n",
    "      iter += 1\n",
    "      # Si un état terminal est visité alors finir l'épisode\n",
    "      if (terminated or truncated or iter > 100):\n",
    "        break\n",
    "    if i%500 == 0:\n",
    "        print(\"\\rEpisode {}/{}.\".format(i, max_episodes), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "  return Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "sqX4MBPV9bYk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 49500/50000."
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=default_frozen_lake, is_slippery=False, render_mode=\"rgb_array\")\n",
    "# Initialiser la fonction action-valeur à zero pour tout (s, a) dans SxA\n",
    "q = {}\n",
    "for s in range(env.observation_space.n):\n",
    " for a in range(env.action_space.n):\n",
    "  q[(s,a)] = 0.0\n",
    "\n",
    "# Appeler la fonction q_learning\n",
    "Qs = q_learning(env, q, epsilon=0.4, gamma=0.9, alpha=0.3, max_episodes=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "y-2Y4WXp_Wlg"
   },
   "outputs": [],
   "source": [
    "def extraire_politique_Qs(env, Qs):\n",
    "  politique = np.zeros(env.observation_space.n)\n",
    "  for state in range(env.observation_space.n):\n",
    "    action = max(list(range(env.action_space.n)), key = lambda x: Qs[(state,x)])\n",
    "    politique[state] = action\n",
    "  return politique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "NjLfShItAaTm"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=default_frozen_lake, is_slippery=False, render_mode=\"human\")\n",
    "politique_q_learning = extraire_politique_Qs(env, Qs)\n",
    "execution_politique(politique_q_learning, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TgdW_11wMsn"
   },
   "source": [
    "### <font color='blue'/> Observer la recompense moyenne par épisode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "CBy2rNqwwMsn"
   },
   "outputs": [],
   "source": [
    "# Q-Learning\n",
    "def q_learning(env,Q_init, epsilon, gamma, alpha, max_episodes=5000):\n",
    "  #Initialiser la fonction action-valeur\n",
    "  Qs = Q_init\n",
    "  G = []\n",
    "  frames = [] # Pour enregistrer les images de l'environnement.\n",
    "  # for each episode\n",
    "  for i in range(max_episodes):\n",
    "    state, _ = env.reset() #initialiser l'environnement au debut de l'épisode\n",
    "    G_episode = 0\n",
    "    pas_episode = 0\n",
    "    while True: # Tant que l'épisode n'est pas fini\n",
    "      # Selectionner une action suivant une politique epsilon-gloutonne\n",
    "      action = epsilon_gloutonne(env, state, Qs, epsilon)\n",
    "      # Entreprendre l'action selectionné et observer la récompense et le nouveau état\n",
    "      sprime, reward, terminated, truncated, info = env.step(action)\n",
    "      # Mettre la fonction action-valeur à jour\n",
    "      Q_max = max(Qs[(sprime, a)] for a in range(env.action_space.n))\n",
    "      Qs[(state, action)] += alpha*(reward + gamma*Q_max - Qs[(state, action)])\n",
    "      G_episode += reward\n",
    "      pas_episode +=1\n",
    "      # remplacer l'état actuelle avec le nouveau état pour continuer à envancer dans l'environnement\n",
    "      state = sprime\n",
    "      # Si un état terminal est visité alors finir l'épisode\n",
    "      if (terminated or truncated):\n",
    "          break\n",
    "\n",
    "      if i%100 == 0 and pas_episode < 100:\n",
    "          frames.append(env.render())\n",
    "    G.append(G_episode)\n",
    "    if i%1000 == 0:\n",
    "      print(\"\\rEpisode {}/{}.\".format(i, max_episodes), end=\"\")\n",
    "  G = np.cumsum(G)/np.arange(1,max_episodes+1)\n",
    "  env.close()\n",
    "  return Qs, G, frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "U5etrTIuwMsn",
    "outputId": "bbc2ea06-06e9-4f0c-ad6d-471e4e6bd5b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000/3000."
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=default_frozen_lake, is_slippery=False, render_mode=\"rgb_array\")\n",
    "\n",
    "# Initialiser la fonction action-valeur à zero pour tout (s, a) dans SxA\n",
    "q = {}\n",
    "for s in range(env.observation_space.n):\n",
    " for a in range(env.action_space.n):\n",
    "  q[(s,a)] = 0.0\n",
    "Qs, G, frames = q_learning(env, q, epsilon=0.5, gamma=0.9, alpha=0.3, max_episodes=3000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDnZ7ksewMsn",
    "outputId": "cf2cb9db-8726-41d6-a4e0-b1452fa580f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to q_learning.avi\n"
     ]
    }
   ],
   "source": [
    "# Une fonction qui enregistre la liste des frames comme video\n",
    "def frames_to_video(frames, fps, video_path):\n",
    "    # Définir le codec et créer un objet VideoWriter\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    frame_size = (frames[0].shape[1], frames[0].shape[0])  # Width x Height\n",
    "    video_writer = cv2.VideoWriter(video_path, fourcc, fps, frame_size)\n",
    "    brg_frames = [cv2.cvtColor(frame, cv2.COLOR_RGB2BGR) for frame in frames]\n",
    "    # Écriture de chaque image dans le fichier vidéo\n",
    "    for frame in brg_frames:\n",
    "        video_writer.write(frame)\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved to {video_path}\")\n",
    "\n",
    "# Appeler la fonction ci-dessus pour enregistrer une video\n",
    "frames_to_video(frames, 10, \"q_learning.avi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "11Y_zaVpwMso",
    "outputId": "2b7421ea-6f3d-44bd-cb89-65a55ba829a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f09771cb670>]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABCvElEQVR4nO3de1xUZf4H8M/MAAPIXWS4CAKCmhdEQRBv1ToJ5ra61S5am2at7Vq2tZQllahZi5m5bunqbvsr23a91G7ZbhmtkmgWat7yjpcwvM1wURjuAzPP7w/06MRFBgfOzPB5v17z6lyeOXzPaWA+nvOc5yiEEAJEREREdkwpdwFEREREN8PAQkRERHaPgYWIiIjsHgMLERER2T0GFiIiIrJ7DCxERERk9xhYiIiIyO4xsBAREZHdc5G7AFswm824ePEivL29oVAo5C6HiIiI2kEIgcrKSoSGhkKpbPscilMElosXLyI8PFzuMoiIiKgDzp07h969e7fZxikCi7e3N4CmHfbx8ZG5GiIiImoPg8GA8PBw6Xu8LU4RWK5dBvLx8WFgISIicjDt6c7BTrdERERk9xhYiIiIyO4xsBAREZHdY2AhIiIiu8fAQkRERHaPgYWIiIjsHgMLERER2T0GFiIiIrJ7DCxERERk9xhYiIiIyO4xsBAREZHdY2AhIiIiu8fAQkRERKisa4DJLPDP3T9g56lSAECjyYyD58rxl+1ncLG8Vtb6nOJpzURERAQIIaBQKNBoMsMsgAaTGQ0mM3zcXbFmxxkszSnA6JieWPXAcLiqlPjyRDEKdJVYue30Tbed/fkJfPnM7Yju5dUFe9IcAwsREZGDq6htwB8+O45/7z8PpVIBY6O51bZfny5D/MtbrP4ZY2ICERXY41bKvCUMLERERA7MbBYYvngLTGZxdYHo0HZmjo7Ehj3nMDqmJx5IjsAja/cCAN6aNgyJkf4I9nGHQqGwVdlWUwghOrZndsRgMMDX1xcVFRXw8fGRuxwiIiKbazSZ8Zcd3+NMSRWMjWZ8d74c5y633q/k2Qn90NNLjXB/T3xxVIctx/TY/twdqG80I2vTEVypacCzE/ojPMADfp5uXbgn11nz/c3AQkREZCcq6xrgpXaBQqGAEAKZHx1GqJ8HZqREYujL/2vzvU/c2RfPTugv61kQa1nz/c1LQkRERHZg1bbTeP2LghbXLd9yss33/mxoKOamDuiMsuwGAwsREZHMThdXtRpWbjQtKRwL7hkEd1cVzGYBpdJxzqbcKgYWIiKiLiCEwFMbDqKyrgH+PdyQHBWASXGhqDWaoF2+HQAQEeAJ7W0avPN1IQDgnYcTkRIdiM8OX0Jcb1/003hL2+tOYQVgHxYiIqJOVVpVj8RXtrar7daMcYgJ8r55QyfBPixEREQyqapvhFkIPLX+ALYVlLT7fSsfGNatwoq1GFiIiIhukRACnx2+hGVfFOBsWU2r7d6aNgza2zQwCQFdRS22nyyFi1KBlL49LS73UHMMLERERLfA2GhGv5c+b3V9zx5u+GTOaPT297RYHhPkzTMqVmBgISIiugUtPYcnPTEcQ3r74lcj+8hQkXNiYCEiIuqARpMZBfpKvJl7Slp2/OU0eLipZKzKeTGwEBFRt1BaVY89hZfRp6cnBob4SCPC1hpNeC3nBIZF+OGeuNCb3i5sNgto/7gd35dUS8uGRfjho9mjHGqUWUfDwEJERE5tf9EV3PvnbyyW3dm/F+obzdj3wxXUX32y8dpvgKc2HIS/pytC/TxwurhKWufr4Ypx/Xrhv99dbPFnvPyzwQwrnYzjsBARkVOqbzThl2vy8d35ik7ZfqCXG+7sH4S5qf0R5OPeKT/D2XEcFiIicnpCCHx79gpOFVfivuG9peHqL5TX4q4/bkddg9mifXSvHvjXb0dh79nLeOz9fRbrHhkdhfQR4Vi17TT+08pZlGvievvigaQIpI8I51mVLsQzLEREZFeMjWa4KBVSX5Kq+kZMWL4dj4yJwqNjomCoa8SCT45g00HLYBER4Imiy83HQAn0csO/Z49Cn549pGVms4BZCKiUCpTXNA2VfzMNJjPKaxrQy1t9i3tI11jz/c3AQkREdqO+0YTJK7/GCV0lAGBkdAB2fX+5w9tb/eBwTBwSYqvyyMas+f5WduQHrFq1CpGRkXB3d0dycjL27NnTatuPPvoIiYmJ8PPzQ48ePRAfH4/333/foo0QAllZWQgJCYGHhwe0Wi1OnTrVyhaJiMgZ7T17Gf1fypHCCoCbhpVXpgxG7jO3WywL9HLDty9qcXbJJIYVJ2J1H5aNGzciIyMDa9asQXJyMlasWIHU1FQUFBQgKCioWfuAgAC8+OKLGDBgANzc3PDpp59i5syZCAoKQmpqKgBg6dKlePPNN/Hee+8hKioK8+fPR2pqKo4dOwZ3d3ZkIiJyRjXGRnx84AIaGs1Y+N9jbbbd9MRoXLhSiyfW7QcAzBwdid/f1Q8+7q4AgLNLJqHRZEZlXWO7Lu+Q47H6klBycjJGjBiBlStXAgDMZjPCw8Px5JNPYt68ee3axvDhwzFp0iQsXrwYQgiEhobimWeewbPPPgsAqKiogEajwdq1azF16tSbbo+XhIiIHMvyLSctBly70YBgb2z+3ViYhcD+onLEh/vBzaVDFwTIznXaJSGj0Yh9+/ZBq9Ve34BSCa1Wi/z8/Ju+XwiB3NxcFBQUYNy4cQCAwsJC6HQ6i236+voiOTm51W3W19fDYDBYvIiIyDH8YfPxFsNKzx5u2PeSFjlPj4NSqYCLSomkqACGFQJg5SWh0tJSmEwmaDQai+UajQYnTpxo9X0VFRUICwtDfX09VCoV/vznP+Ouu+4CAOh0OmkbP97mtXU/lp2djUWLFllTOhERychsFnhjSwH+891FnLtcKy3PvncIhkf4o5e3GgG8lENt6JJxWLy9vXHw4EFUVVUhNzcXGRkZiI6Oxh133NGh7WVmZiIjI0OaNxgMCA8Pt1G1RERkC/WNJvx+40FsPtzyPz7zM3+CEF+PLq6KHJVVgSUwMBAqlQp6vd5iuV6vR3BwcKvvUyqViImJAQDEx8fj+PHjyM7Oxh133CG9T6/XIyTkem9uvV6P+Pj4FrenVquhVvM+eCIie1NR24DKugb89K2dKK9paLXdv2enMKyQVawKLG5ubkhISEBubi6mTJkCoKnTbW5uLubMmdPu7ZjNZtTX1wMAoqKiEBwcjNzcXCmgGAwG7N69G7Nnz7amPCIi6kRCCHx1qhSBXmrcFuINAKhvNOPLE8V4/J/7b/r+WWOj8OiYaAT78u5Psp7Vl4QyMjIwY8YMJCYmIikpCStWrEB1dTVmzpwJAJg+fTrCwsKQnZ0NoKm/SWJiIvr27Yv6+nps3rwZ77//PlavXg0AUCgUePrpp/HKK68gNjZWuq05NDRUCkVERGQbq7adRlRgD9zdzvFJGk1m5BzVYc66Ax36eX+aGo/YIG/EarzgqmLnWeo4qwNLeno6SkpKkJWVBZ1Oh/j4eOTk5EidZouKiqBUXv9QVldX4/HHH8f58+fh4eGBAQMG4B//+AfS09OlNs899xyqq6vx2GOPoby8HGPGjEFOTg7HYCEishEhBKIyN7e4blpSOH43PhZ1DWYU6CqhvS0ILiol/rjlJP7Uyq3HrfFxd8HPh4VhcJgvfpHIvoVkOxyan4jIwZ0uroSfpxuq6xuRf6YM8z46DAAYGOKDjx4fhcvVRoxa8qVNftbY2EBMHByCjd8W4UJ5LXqoXRDs4443fjkUvf09bfIzqPvgs4SIiLqJD/aew3P/OmTVe343PrbVQdtacnu/Xvhjejx83F3gwss6ZEPWfH93yW3NRERkOz+UVeOnb+5En0BPHLlg3cCZp16dCFeVEhl39QMA1DWYcP5KDf649RQ+P3wJi6cMxqL/HIPRZAYA7Jh7JyJ68swJyY9nWIiIHEBdgwlGkxl/2/E93vzydKvtHhrZB6mDgjEmNhDfnr2MX6xpGjH8Dz8fggeSI7qqXKJ24RkWIiInUms04basnFbX/2fOaMT19mu2fERkAM4umdSJlRF1HQYWIiI7daG8FqNb6Sz75weHw8/DFX2DvKDx4R2V5PwYWIiI7NC63UV44ePDLa5jvxLqjhhYiIjszF93nMEfNls+UDbMzwM7n78TCoVCpqqI5MXAQkQkAyEEPth7Dn17eSExMgDfnr0Mf09XxAR5Y8O356R201P64OXJg2WslMg+MLAQEXWB0qp6XK42op+m6Rk8eSdL8Py/W77kc81Xz92J8ABe+iECGFiIiDpdrdGExFe2AgBG9e2JlycPwso2bk0GgDv792JYIboBAwsRUSeoazDhTEkV3F1VmPX3vdLyb86UQbt8hzTvpXZBVX0jkqIC0LeXF9bvKYJCASy5L06OsonsFgMLEVEneODtXdhfVN5mm17eanz7otZiWfa9QzqxKiLHxcBCRGRjRy5UtBhWCrPvht5Qj+VbCnClpgGv/pydaYnai4GFiKiDymuMWPvNWRy5UIGntf0wOMwXxYY6/PStnVKbyfGhOHS+Aq9OGQyFQoFgX3csvX+ojFUTOSYGFiKiDth6TI9f39A3ZevxYkyKC8Fnhy5Jy974xVDcl9BbjvKInA4DCxGRFRpNZmw/WWIRVq65Maxob9Pg3uFhXVkakVNjYCEiaichBGJe/Nxi2a/HROGeoaF4/J/7caG8Vlq++lfDOSotkQ0xsBARtcPp4ipol2+3WPb4HX3xXNoAAEDO02Px2aFLmBQXAm93VzlKJHJqDCxERDdhMotmYeXvjyRhXL9e0ry3uyumJkV0dWlE3QYDCxFRG2qMjRiY9YXFMg6ZT9T1GFiIiNrw1g1D6IcHeOCr534iYzVE3RcDCxFRC8qq6nHskgGr885Iyz6aPVrGioi6NwYWIqIbNJrM+OVf8puNVHt0USp6qPknk0gu/O0jIgLw1IYD+OTgxRbXjYj0Z1ghkhl/A4mo2yoqq8Fj7+/FCV1li+t7uKnw6s+HYMowDgBHJDcGFiLqVhpMZhy7aMDD7+7BlZqGFtu4uShxaMEEuLuqurg6ImoNAwsROb1NBy7g6Y0Hb9pua8bt0BvqkBQVAFeVsvMLI6J2Y2AhIodnbDTj7/ln8cHec9DepsGh8xXYebq0Xe/1dFPhyMJUKJVNw+jHBHl1ZqlE1EEMLETk0IQQ6PfS9ef7nNRX3fQ9g8N88HzaAIyN7XXTtkRkHxhYiMih1DWYUKCrxCufHcN9w3vju/MV7XrfP3+djEGhPvBSu8CFl3uIHA4DCxHZvddyTlgM4HbNt2evWMz/e3YKtheU4M0vT+Oz343BwBAfNJoF+6MQOQEGFiKyW3sKL+OkvrLFsPJjm54YjfhwPyT0CUDGhP7ScleVojNLJKIuwsBCRHZHCIGozM03bfeb26OxbncRPpo9CrEa7y6ojIjkwsBCRHanpYHctLdpsPpXw+GqUqKorAahfu5wUSmROfE2GSokoq7GwEJEdkMIgQfe3o3878ukZZkTB2B0TCAGh/lKyyJ6espRHhHJiIGFiOzGlmN6i7Dy8uRBmJ4SKV9BRGQ32HWeiOzGks9PWMxPHspn+BBRE55hISK7cO5yDb4vrQYAvHbfEKSPiJC5IiKyJzzDQkR24bPDl6Tp+xPCZayEiOwRz7AQkayEEJiy6mtpxNoX774NKiXHTiEiSwwsRCSbAl0lNh++ZDG8/p0D+HwfImquQ5eEVq1ahcjISLi7uyM5ORl79uxpte3bb7+NsWPHwt/fH/7+/tBqtc3aP/zww1AoFBavtLS0jpRGRA6i2FCH1BU78KfcU9KyF+++DTFBHACOiJqzOrBs3LgRGRkZWLBgAfbv34+hQ4ciNTUVxcXFLbbPy8vDtGnTsG3bNuTn5yM8PBwTJkzAhQsXLNqlpaXh0qVL0mv9+vUd2yMicgh5BSUW8/+enYJZ46JlqoaI7J1CCCGseUNycjJGjBiBlStXAgDMZjPCw8Px5JNPYt68eTd9v8lkgr+/P1auXInp06cDaDrDUl5ejk2bNlm/BwAMBgN8fX1RUVEBHx+fDm2DiLrOov8exbtfn5Xmn/xJDJ654fk/RNQ9WPP9bdUZFqPRiH379kGr1V7fgFIJrVaL/Pz8dm2jpqYGDQ0NCAgIsFiel5eHoKAg9O/fH7Nnz0ZZWVkrWwDq6+thMBgsXkRkf2qMjfjmTCnOX6nBxfJa1BpNeO5f31mElU1PjGZYIaKbsqrTbWlpKUwmEzQajcVyjUaDEydOtPIuS88//zxCQ0MtQk9aWhruvfdeREVF4cyZM3jhhRcwceJE5OfnQ6VSNdtGdnY2Fi1aZE3pRNTFhBAYmPVFm20eGxeN+HC/rimIiBxal94ltGTJEmzYsAF5eXlwd3eXlk+dOlWaHjJkCOLi4tC3b1/k5eVh/PjxzbaTmZmJjIwMad5gMCA8nOM2ENkLIQT+suP7NtssnjIYD43s00UVEZGjsyqwBAYGQqVSQa/XWyzX6/UIDg5u873Lli3DkiVLsHXrVsTFxbXZNjo6GoGBgTh9+nSLgUWtVkOtVltTOhF1kffzz+K1nAJU1TdKy2KDvFBZ1widoQ4AsPP5O9Hbnw8wJKL2syqwuLm5ISEhAbm5uZgyZQqApk63ubm5mDNnTqvvW7p0KV599VV88cUXSExMvOnPOX/+PMrKyhASEmJNeURkB97YctIirKyfNRIpfXvKWBEROQOrb2vOyMjA22+/jffeew/Hjx/H7NmzUV1djZkzZwIApk+fjszMTKn9a6+9hvnz5+Odd95BZGQkdDoddDodqqqqAABVVVWYO3cudu3ahbNnzyI3NxeTJ09GTEwMUlNTbbSbRNTZDhRdQeS8z1Be02CxnGGFiGzB6j4s6enpKCkpQVZWFnQ6HeLj45GTkyN1xC0qKoJSeT0HrV69GkajEffff7/FdhYsWICFCxdCpVLh0KFDeO+991BeXo7Q0FBMmDABixcv5mUfIjt07nINXv3sOGaMikRK354QQuAPm4/j7a8KLdqt+3UykqICWtkKEZF1rB6HxR5xHBairjPv34ew4dtzbbb54DcpDCtEdFPWfH/zWUJE1C7lNUZsOnChzbDy+B19MTe1PxQKPryQiGyLgYWI2iX+5S1trl/zq+FIG8yO8kTUORhYiKhVxy4aEBXYAw/8bZfF8ruHBOOp8f3wycELeCilDwK91HBVdehZqkRE7cLAQkQtev2LE1i17Uyz5b/7SQwevzMG7q4qPJc2QIbKiKg7YmAhomaEEC2GlS2/H4dYjbcMFRFRd8dzuETUTIG+stmyBfcMZFghItnwDAsRNbOn8LI0/aep8dj1/WU8kBwhY0VE1N0xsBCR5Ieyajz4t904f6UWAPB7bT9Mjg/D5PgwmSsjou6OgYWIAAC7vi/D1L9a3g00rl+gTNUQEVliHxYiAgDM/sc+i/lfj4nCsAh/maohIrLEMyxEhLKqely5+tBChQL48pk7EBXYQ+aqiIiuY2Ah6uaKK+uQ9GquNH9icRrULioZKyIiao6XhIi6sR+HlT49PRlWiMguMbAQdVONJjPu/tNXFsvWzxopUzVERG3jJSGibuqErhKlVcbr84vT4O7KsytEZJ94hoWom9r3wxVp+o1fDGVYISK7xsBC1A0dPl+BBf85CgDIuKsf7kvoLXNFRERtY2Ah6oae3nhAmk7ow7FWiMj+MbAQdTOfHLyAMyXV0vzI6J4yVkNE1D7sdEvUTVyuNmL+piP47PAladnRRalQKRUyVkVE1D4MLETdxMPv7sGh8xXSfHpiOHqo+SeAiBwD/1oROYGK2gac0leixmjCmJhAKH901qSitsEirPwysTdeuz+uq8skIuowBhYiB7fyy1NY9r+TFsumJUVg/Z4iAMCOuXfiTEmVtO4vDyVgbCyfwkxEjoWBhciB1TWYmoUVAFJYAYBxr2+TpifHhyJ1UHCX1EZEZEu8S4jIgQ2Yn2Mx76pquwMt7wgiIkfFMyxEDupKtdFi/uySSQCAwtJq5B7X47vzFUgdpMGcddfHXLlvOAeIIyLHxMBC5GDezD2F93f9gJLKemnZkUWp0nRUYA/8emy0ND8swh8qhQLBvu5dWicRkS0xsBA5kLv/9BWOXTJYLPNwVcGrjduTw/w8OrssIqJOxz4sRA5iW0Fxs7ACAIcXTpChGiKirsUzLEQO4ExJFWa++600//r9cdAb6jA03A8uKv67g4icHwMLkR27XG3Ea5+fwOYbhtP3dFPhF4nhMlZFRNT1GFiI7Njf889i495zFsv2vKiVqRoiIvnwXDKRHdv3wxWL+TW/Gt5mB1siImfFv3xEdupsaTW+OlUKABgQ7I2ntbFIGxwic1VERPJgYCGyU58euihNf/jbFHi7u8pYDRGRvHhJiMhO7P6+DD9buRNfn246q3LtGUFjYwMZVoio2+MZFiI78dD/7YHRZMaDf9ttsfw34/rKVBERkf3gGRYiO1BrNMFoMre4bkxsYBdXQ0RkfxhYiGQmhMC6PUUtrvv0yTFdXA0RkX3iJSEimW0/WYLFnx4DAIyMDsBLkwbina8L8auRfTA4zFfm6oiI7AMDC5HMdl69dRkA+mu8MTjMF8t/GS9fQUREdqhDl4RWrVqFyMhIuLu7Izk5GXv27Gm17dtvv42xY8fC398f/v7+0Gq1zdoLIZCVlYWQkBB4eHhAq9Xi1KlTHSmNyOFsKyiWph8ZEyVjJURE9svqwLJx40ZkZGRgwYIF2L9/P4YOHYrU1FQUFxe32D4vLw/Tpk3Dtm3bkJ+fj/DwcEyYMAEXLlyQ2ixduhRvvvkm1qxZg927d6NHjx5ITU1FXV1dx/eMyAH8UFaNMyXVAIB/zx6FPj17yFwREZF9UgghhDVvSE5OxogRI7By5UoAgNlsRnh4OJ588knMmzfvpu83mUzw9/fHypUrMX36dAghEBoaimeeeQbPPvssAKCiogIajQZr167F1KlTb7pNg8EAX19fVFRUwMfHx5rdIZJV9ubj+MuO7wEABa+kQe2ikrkiIqKuY833t1VnWIxGI/bt2wet9vrD15RKJbRaLfLz89u1jZqaGjQ0NCAgIAAAUFhYCJ1OZ7FNX19fJCcnt7rN+vp6GAwGixeRI8o5qgMA3Nm/F8MKEVEbrAospaWlMJlM0Gg0Fss1Gg10Ol27tvH8888jNDRUCijX3mfNNrOzs+Hr6yu9wsPDrdkNIruw81QpfiirAQA8mNxH5mqIiOxbl47DsmTJEmzYsAEff/wx3N3dO7ydzMxMVFRUSK9z587ZsEqirrH+hrFXODgcEVHbrLqtOTAwECqVCnq93mK5Xq9HcHBwm+9dtmwZlixZgq1btyIuLk5afu19er0eISHXn0Sr1+sRHx/f4rbUajXUarU1pRPZnc8OXwIAPDI6Cu6uvBxERNQWq86wuLm5ISEhAbm5udIys9mM3NxcpKSktPq+pUuXYvHixcjJyUFiYqLFuqioKAQHB1ts02AwYPfu3W1uk8iR3fgk5qlJvKRJRHQzVg8cl5GRgRkzZiAxMRFJSUlYsWIFqqurMXPmTADA9OnTERYWhuzsbADAa6+9hqysLKxbtw6RkZFSvxQvLy94eXlBoVDg6aefxiuvvILY2FhERUVh/vz5CA0NxZQpU2y3p0R2YvvJEsxZd0Ca76fxlrEaIiLHYHVgSU9PR0lJCbKysqDT6RAfH4+cnByp02xRURGUyusnblavXg2j0Yj777/fYjsLFizAwoULAQDPPfccqqur8dhjj6G8vBxjxoxBTk7OLfVzIbJHF8trMeOd6wMnpg1q+1IqERE1sXocFnvEcVjIUUTO+0ya7u3vgZ3P/0TGaoiI5NVp47AQUceV1xgt5nfMvVOmSoiIHA8DC1EXGZl9vWP56VcnQqlUyFgNEZFjYWAh6gK1RhPqGswAgJ493OCi4q8eEZE1+FeTqJOdLq7EbVk50vzmp8bKWA0RkWNiYCHqZF8ctRxoUePDu9+IiKxl9W3NRNQ+20+WWNzCDADbnr1DnmKIiBwcz7AQdYL6RlOzsPKXhxIQFdhDpoqIiBwbAwtRJ/i+pNpiXqkAtLdpWmlNREQ3w0tCRJ3gH7t+kKY/fnwUhkX4y1gNEZHj4xkWok5w7UnMiX38GVaIiGyAgYXIxmqMjSivaQAAPDomSuZqiIicAwMLkQ0du2jAwKwvpPlx/XrJWA0RkfNgYCGykRpjI+5+8ytpPtjHHT3U7CZGRGQLDCxENpJ7vNhiftWDw2SqhIjI+fCff0Q2UF5jxJPrD0jze14YjyCOaEtEZDM8w0JkAxu/PSdN/zKxN8MKEZGNMbAQ2UD25yek6cVTBstYCRGRc2JgIbpFFbUN0vTr98dB7aKSsRoiIufEwEJ0i/721ffS9OT4MBkrISJyXgwsRLdACIF3vz4rzbu58FeKiKgz8K8r0S1475uzqKpvBACs+VWCzNUQETkvBhaiDjpdXIWF/z0mzcf19pWxGiIi58bAQtQBDSYztMu3S/MPJEcg1M9DxoqIiJwbAwuRldbvKULsi59bLMucOECmaoiIugcGFiIrCCGQ+dFhi2WbfzcW3u6uMlVERNQ9cGh+onYwNprx3flyvJl7ymL5i3ffhoGhPjJVRUTUfTCwEN1EjbERA7O+aLZ8zp0xmDEqsusLIiLqhhhYiG5i2Rcnmy374DcpSIoKkKEaIqLuiX1YiNpQoKvEO18XWiz7vbYfwwoRURfjGRaiNtyzcqc0vXjKYDw0so+M1RARdV88w0LUCr2hDsZGMwDAw1XFsEJEJCOeYSFqwYqtJ7Fi6/U7gr59SStjNURExMBCdIPKugZMenMnii7XWCz3UvNXhYhITrwkRHSDTQcuNAsr38z7iUzVEBHRNfxnI9ENThVXSdPDIvywftZIuLuqZKyIiIgABhYiC3/P/wEAcEf/Xnh7eiJcVTwJSURkD/jXmOiqWqNJmn50TBTDChGRHeFfZKKrUlfskKbHxATKWAkREf0YAwsRmh5ueGNnW4VCIWM1RET0YwwsRAAKS6ul6f/OGSNjJURE1BIGFiIAi/57VJoe0ttXxkqIiKglHQosq1atQmRkJNzd3ZGcnIw9e/a02vbo0aO47777EBkZCYVCgRUrVjRrs3DhQigUCovXgAEDOlIaUbsdvViBHSdL8OHec/jmTBkAQMkrQUREdsnq25o3btyIjIwMrFmzBsnJyVixYgVSU1NRUFCAoKCgZu1ramoQHR2NX/ziF/j973/f6nYHDRqErVu3Xi/MhXdcU+cRQmDmu9+iuLLeYvn/zRghU0VERNQWq8+wLF++HLNmzcLMmTMxcOBArFmzBp6ennjnnXdabD9ixAi8/vrrmDp1KtRqdavbdXFxQXBwsPQKDORdGtQ59hddwSNrm4eVYB933NG/l0xVERFRW6wKLEajEfv27YNWe/1BcEqlElqtFvn5+bdUyKlTpxAaGoro6Gg8+OCDKCoqarVtfX09DAaDxYuoPUoq63Hvn7/BtoKSZuvyM3/Cu4OIiOyUVYGltLQUJpMJGo3GYrlGo4FOp+twEcnJyVi7di1ycnKwevVqFBYWYuzYsaisrGyxfXZ2Nnx9faVXeHh4h382dS+HL5Q3WzYpLgRvThvGsEJEZMfsoqPIxIkTpem4uDgkJyejT58++OCDD/Doo482a5+ZmYmMjAxp3mAwMLRQuxSVXR9r5a6BGsxN7Y9+Gm8ZKyIiovawKrAEBgZCpVJBr9dbLNfr9QgODrZZUX5+fujXrx9Onz7d4nq1Wt1mfxii1iz87zEAwEMj+2DxlMEyV0NERO1l1SUhNzc3JCQkIDc3V1pmNpuRm5uLlJQUmxVVVVWFM2fOICQkxGbbJDp3w0i2/j3cZKyEiIisZfUloYyMDMyYMQOJiYlISkrCihUrUF1djZkzZwIApk+fjrCwMGRnZwNo6qh77NgxafrChQs4ePAgvLy8EBMTAwB49tlncc8996BPnz64ePEiFixYAJVKhWnTptlqP4mwv+iKNP3wqEj5CiEiIqtZHVjS09NRUlKCrKws6HQ6xMfHIycnR+qIW1RUBKXy+ombixcvYtiwYdL8smXLsGzZMtx+++3Iy8sDAJw/fx7Tpk1DWVkZevXqhTFjxmDXrl3o1Yu3mJJtlFTW46kNBwEAA4K9EcAzLEREDkUhhBByF3GrDAYDfH19UVFRAR8fH7nLITsUOe8zafqeoaF4a9qwNloTEVFXsOb7m88SIqdXVmU5QNwLd/OxD0REjoaBhZxe8h+udxLfPvcOhPh6yFgNERF1BAMLObWKmgY0mq9f9ezTs4eM1RARUUcxsJBT+/f+89L0x4+PkrESIiK6FQws5NRe/rTplno3FyWGRfjLXA0REXWUXQzNT2RrVfWNWP6/k9L8A0kRMlZDRES3imdYyCm9981ZvPN1oTS/8GeDZKyGiIhuFQMLOaW9Zy9L0/00XjJWQkREtsDAQk7n69Ol2FZQIs3/34wRMlZDRES2wMBCTueVz45L0x8/PgrhAZ4yVkNERLbAwEJOxdhoxvFLBmk+uhcvBxEROQMGFnIqW4/rpel//TYFvh6uMlZDRES2wsBCTuVsWbU0nRgZIGMlRERkSwws5FQuldcBAJ64s6/MlRARkS1x4DhyCmazwK7CMry/6wcAQKgfH3BIRORMGFjIKby6+Tj+b+f1geIi+ZBDIiKnwktC5PBqjI0WYQUARkb3lKkaIiLqDAws5PDW5J2Rpj1cVfh63k+gUipkrIiIiGyNl4TI4Z2/UitNH1+cJmMlRETUWXiGhRxaWVU9PjpwAQDwypTBMldDRESdhYGFHFajyYyEV7ZK8xyCn4jIeTGwkMP66lSpxfzovuxoS0TkrBhYyGGdu1IjTX/wmxS4qPhxJiJyVvwLTw7rUkXTqLYPj4pEUhSH4ScicmYMLOSwLpU33R0U4usucyVERNTZGFjIYV28eoYlhMPwExE5PQYWcliXKprOsITyDAsRkdNjYCGHZDYLnLvcFFiCGViIiJweAws5pEX/PSpNa3wYWIiInB0DCzmkbQUl0rQrb2cmInJ6/EtPDsnDVQUAWPSzQTJXQkREXYGBhRzSxasdbkdxdFsiom6BgYUcTlV9IyrrGgHwlmYiou6CgYUcju7q2RVvtQu81C4yV0NERF2BgYUcjnb5DgCASQiZKyEioq7CwEIOpbKuQZoeHOorYyVERNSVGFjIofw9/wdpet7dA2SshIiIuhIDCzmUP245KU0Pj/CXsRIiIupK7LFIDsHYaMbyLSfRaG7qtzJrbJTMFRERUVdiYCGH8MpnxywuBz06JlrGaoiIqKvxkhA5hBvDire7C4K81TJWQ0REXa1DgWXVqlWIjIyEu7s7kpOTsWfPnlbbHj16FPfddx8iIyOhUCiwYsWKW94mdS9V9Y3S9H3De+PQgglQKhUyVkRERF3N6sCyceNGZGRkYMGCBdi/fz+GDh2K1NRUFBcXt9i+pqYG0dHRWLJkCYKDg22yTepedBV10vQbvxwKhYJhhYiou7E6sCxfvhyzZs3CzJkzMXDgQKxZswaenp545513Wmw/YsQIvP7665g6dSrU6pZP41u7Tepezl+pAQDEBHnJXAkREcnFqsBiNBqxb98+aLXa6xtQKqHVapGfn9+hAjqyzfr6ehgMBosXOSchBB5+91sAgIpnVoiIui2rAktpaSlMJhM0Go3Fco1GA51O16ECOrLN7Oxs+Pr6Sq/w8PAO/Wyyfx8fuCBNhwd4ylgJERHJySHvEsrMzERFRYX0OnfunNwlUSfJ+OA7aXrp/XEyVkJERHKyahyWwMBAqFQq6PV6i+V6vb7VDrWdsU21Wt1qfxhyTpPiQhDQw03uMoiISCZWnWFxc3NDQkICcnNzpWVmsxm5ublISUnpUAGdsU1yHq6qpn4rv9fGylwJERHJyeqRbjMyMjBjxgwkJiYiKSkJK1asQHV1NWbOnAkAmD59OsLCwpCdnQ2gqVPtsWPHpOkLFy7g4MGD8PLyQkxMTLu2Sd1TVX0jGkxNQ/EH+3rIXA0REcnJ6sCSnp6OkpISZGVlQafTIT4+Hjk5OVKn2aKiIiiV10/cXLx4EcOGDZPmly1bhmXLluH2229HXl5eu7ZJ3dOZ4ipp2kvNp0gQEXVnCiGEkLuIW2UwGODr64uKigr4+PjIXQ7ZyJu5p7D86tOZzy6ZJHM1RERka9Z8fzvkXULUPdQ2mAAAg0IZQomIujsGFrJLQgiszjsDAEgb1LE70IiIyHkwsJBdKq9pkKYjenLAOCKi7o6BhezSjlMl0vRP40JlrISIiOwBAwvZpe9LqgEAahclVEo+Q4iIqLtjYCG7U2yow59yTwEA7h3eW+ZqiIjIHjCwkN355OBFaTo2yEvGSoiIyF4wsJBdqa5vxKubj0vzU5P4JG4iImJgITuzu7BMmv7NuGh4unGEWyIiYmAhO6M31EvTM0ZFylcIERHZFQYWsitrtjcNFnfvsDCE+vGBh0RE1ISBheyKStF0C3O9ySxzJUREZE8YWMhunC2txvelTeOvPJAUIXM1RERkTxhYyG5k/eeoNB3s6y5jJUREZG8YWMhu7Dpz/Q6hMPZfISKiGzCwkF2oazDBeLXfyrpZyXB3VclcERER2RMGFpLdjpMlGDA/R5of2ttPvmKIiMguMbCQ7DYduGAx30PNweKIiMgSAwvJ7sjFCmn6j+lDZayEiIjsFf8pS7L69NBFnNRXAQDW/Go40gaHyFwRERHZI55hIVkdOn/97EqsxlvGSoiIyJ4xsJCsSquanh30kwFB6NvLS+ZqiIjIXjGwkKxKq4wAgImDg2WuhIiI7BkDC8nmP99dxI6TJQCAXt5qmashIiJ7xsBCsln86TFpmoGFiIjawsBCshBCoKSyqf+Kn6crbgv2kbkiIiKyZwwsJIviq2EFAHIzbodSqZCxGiIisncMLCSLvWevSNM9vXg5iIiI2sbAQrJYv6cIAPuuEBFR+zCwkCx8PV0BACG+7jJXQkREjoCBhWRRUdMAAHh4VKS8hRARkUNgYCFZXBvhNpD9V4iIqB0YWEgW10a47enlJnMlRETkCBhYqMuZzAKXq5vOsPTiGRYiImoHBhbqcldqjDCLpmn/HjzDQkREN8fAQl2u7OrlIH9PV7iq+BEkIqKb47cFdblnPjwIAHBz4cePiIjah98Y1OXOX6kFAOmyEBER0c0wsFCXUymanhu0ctowmSshIiJHwcBCXcrYaEZZdVMflqhePWSuhoiIHAUDC3WZy9VGTFn1tTQf4Mk7hIiIqH06FFhWrVqFyMhIuLu7Izk5GXv27Gmz/YcffogBAwbA3d0dQ4YMwebNmy3WP/zww1AoFBavtLS0jpRGduyTgxdw7JJBmnfhHUJERNROVn9jbNy4ERkZGViwYAH279+PoUOHIjU1FcXFxS22/+abbzBt2jQ8+uijOHDgAKZMmYIpU6bgyJEjFu3S0tJw6dIl6bV+/fqO7RHZrUX/PSZNP35HXxkrISIiR6MQQlh1r0ZycjJGjBiBlStXAgDMZjPCw8Px5JNPYt68ec3ap6eno7q6Gp9++qm0bOTIkYiPj8eaNWsANJ1hKS8vx6ZNmzq0EwaDAb6+vqioqICPj0+HtkGd60q1EcMWbwEAPDomCi9Nug2Kq51viYioe7Lm+9uqMyxGoxH79u2DVqu9vgGlElqtFvn5+S2+Jz8/36I9AKSmpjZrn5eXh6CgIPTv3x+zZ89GWVlZq3XU19fDYDBYvMi+rdtTJE3PuTOGYYWIiKxiVWApLS2FyWSCRqOxWK7RaKDT6Vp8j06nu2n7tLQ0/P3vf0dubi5ee+01bN++HRMnToTJZGpxm9nZ2fD19ZVe4eHh1uwGdbEGkxmvf1EgzXM4fiIispaL3AUAwNSpU6XpIUOGIC4uDn379kVeXh7Gjx/frH1mZiYyMjKkeYPBwNBipw6dL8fPVl6/M+iP6UNlrIaIiByVVWdYAgMDoVKpoNfrLZbr9XoEBwe3+J7g4GCr2gNAdHQ0AgMDcfr06RbXq9Vq+Pj4WLzIPm07UWIxP3lomEyVEBGRI7MqsLi5uSEhIQG5ubnSMrPZjNzcXKSkpLT4npSUFIv2ALBly5ZW2wPA+fPnUVZWhpCQEGvKIzu0atv10Jn104FQKtl3hYiIrGf1bc0ZGRl4++238d577+H48eOYPXs2qqurMXPmTADA9OnTkZmZKbV/6qmnkJOTgzfeeAMnTpzAwoULsXfvXsyZMwcAUFVVhblz52LXrl04e/YscnNzMXnyZMTExCA1NdVGu0lyMZrMAIAn7uyLR8ZEyVwNERE5Kqv7sKSnp6OkpARZWVnQ6XSIj49HTk6O1LG2qKgISuX1HDRq1CisW7cOL730El544QXExsZi06ZNGDx4MABApVLh0KFDeO+991BeXo7Q0FBMmDABixcvhlqtttFukhzqG693mtbepmmjJRERUdusHofFHnEcFvt0sbwWo5Z8CQA484e7oeLlICIiukGnjcNCZI3LVx9yqPFRM6wQEdEtYWChTrP406ah+L3UdnH3PBEROTAGFuo0uwsvA7je8ZaIiKijGFioU5y7XCNNL7k3TsZKiIjIGTCwUKc4qa+UphP6+MtYCREROQMGFuoU1zrc3t6vF9xdVTJXQ0REjo6BhTrFsUtNT9DuyQcdEhGRDTCwUKfYdqJY7hKIiMiJMLBQp/C/emYlVuMtcyVEROQMGFioU1TUNAAAhkX4yVsIERE5BQYWsrkfyqrxfWk1ACCAfViIiMgGGFjI5j4/opOmNd7uMlZCRETOgoGFbG7J5ycAAP00XvD1dJW5GiIicgYMLGRzHlfHXZk0JFTmSoiIyFkwsJBNNZrMqG0wAQAeHBkhczVEROQsGFjIpsprG6RpPw9eDiIiIttgYCGbOnqxaYTbHm4quKj48SIiItvgNwrZTGVdA97KPQUAqDaaZK6GiIicCQML2czar89i7w9XAAD3De8tczVERORMGFjIZn64XCNN/zQuRMZKiIjI2TCwkM2U1xgBAH/4+RDcOSBI5mqIiMiZMLCQzWw93vSEZn8OFkdERDbGwEI2cbG8VpoO8lHLWAkRETkjBhayidwTxdL08Ah/GSshIiJnxMBCNvH3b84CAML8PKBQKOQthoiInI6L3AWQY/v6dCme+9chlFTWAwAeSukjc0VEROSMGFjolmw+fAkXbui/MmGgRsZqiIjIWTGw0C259uyguwZq8NO4EET38pK5IiIickbsw0K3pKKmKbDcPSQYk+PDZK6GiIicFQMLdZgQAjtPlwIA/DzcZK6GiIicGQMLddip4ippOjKwh4yVEBGRs2NgoQ65Um3EPW/tlOajGFiIiKgTMbBQh2w5rkd9oxkAcEf/XjJXQ0REzo6BhTrkv99dlKZfmTJYxkqIiKg7YGChDiksrQYA3De8N3r7e8pcDREROTsGFrLaKX0lzl9pGizuvgTeykxERJ2PgYWs9voXBdJ0dCAHiiMios7HwEJW0199btBdAzUI9nWXuRoiIuoOGFjIaoarw/HPGhstcyVERNRd8FlC1C5nSqpwsbwWvh6uuFxtBAD4erjKXBUREXUXDCx0UyWV9UhbsQMNJmGxnIGFiIi6SocCy6pVq/D6669Dp9Nh6NCheOutt5CUlNRq+w8//BDz58/H2bNnERsbi9deew133323tF4IgQULFuDtt99GeXk5Ro8ejdWrVyM2NrYj5RGA5f8rwD92F6GHWgUfd1d4u7tc/a8rfDxcLJZ5uKng6aa6+l+XpmnX68uKLlejwSTgqlIg0EuN8poGDIvwQ5C3Wu7dJCKibsLqwLJx40ZkZGRgzZo1SE5OxooVK5CamoqCggIEBQU1a//NN99g2rRpyM7Oxk9/+lOsW7cOU6ZMwf79+zF4cNOAY0uXLsWbb76J9957D1FRUZg/fz5SU1Nx7NgxuLuzU2dHfLjvPC5XG3G5GgBqbbLN/sHe+PTJsTbZFhERkTUUQghx82bXJScnY8SIEVi5ciUAwGw2Izw8HE8++STmzZvXrH16ejqqq6vx6aefSstGjhyJ+Ph4rFmzBkIIhIaG4plnnsGzzz4LAKioqIBGo8HatWsxderUm9ZkMBjg6+uLiooK+Pj4WLM7Tmvoov+horYBb04bBh93F1TWNcJQ19D039oGabqyrhE1xkbUGk2oufqqbTChxtiIugazxTanp/TBy5M5qi0REdmGNd/fVp1hMRqN2LdvHzIzM6VlSqUSWq0W+fn5Lb4nPz8fGRkZFstSU1OxadMmAEBhYSF0Oh20Wq203tfXF8nJycjPz28xsNTX16O+vl6aNxgM1uxGuzWazHh18/FO2XZnq6pvBACMiPRHiK9Hh7ZhNour4cWEBpMZIbyFmYiIZGJVYCktLYXJZIJGo7FYrtFocOLEiRbfo9PpWmyv0+mk9deWtdbmx7Kzs7Fo0SJrSu8QswDe/fpsp/+czuKqUsDHveMdY5VKBXqoXdBDzb7ZREQkL4f8JsrMzLQ4a2MwGBAeHm7zn6NUAE/c2dfm2+0qwyP8GTaIiMgpWPVtFhgYCJVKBb1eb7Fcr9cjODi4xfcEBwe32f7af/V6PUJCQizaxMfHt7hNtVoNtbrz71BxUSkxN3VAp/8cIiIiaptVI926ubkhISEBubm50jKz2Yzc3FykpKS0+J6UlBSL9gCwZcsWqX1UVBSCg4Mt2hgMBuzevbvVbRIREVH3YvX1goyMDMyYMQOJiYlISkrCihUrUF1djZkzZwIApk+fjrCwMGRnZwMAnnrqKdx+++144403MGnSJGzYsAF79+7FX//6VwCAQqHA008/jVdeeQWxsbHSbc2hoaGYMmWK7faUiIiIHJbVgSU9PR0lJSXIysqCTqdDfHw8cnJypE6zRUVFUCqvn7gZNWoU1q1bh5deegkvvPACYmNjsWnTJmkMFgB47rnnUF1djcceewzl5eUYM2YMcnJyOAYLERERAejAOCz2iOOwEBEROR5rvr/5tGYiIiKyewwsREREZPcYWIiIiMjuMbAQERGR3WNgISIiIrvHwEJERER2j4GFiIiI7B4DCxEREdk9BhYiIiKye1YPzW+Prg3WazAYZK6EiIiI2uva93Z7Bt13isBSWVkJAAgPD5e5EiIiIrJWZWUlfH1922zjFM8SMpvNuHjxIry9vaFQKGy6bYPBgPDwcJw7d47PKboJHqv247FqPx4r6/B4tR+PVft11rESQqCyshKhoaEWD05uiVOcYVEqlejdu3en/gwfHx9+oNuJx6r9eKzaj8fKOjxe7cdj1X6dcaxudmblGna6JSIiIrvHwEJERER2j4HlJtRqNRYsWAC1Wi13KXaPx6r9eKzaj8fKOjxe7cdj1X72cKycotMtEREROTeeYSEiIiK7x8BCREREdo+BhYiIiOweAwsRERHZPQaWNqxatQqRkZFwd3dHcnIy9uzZI3dJXW7hwoVQKBQWrwEDBkjr6+rq8MQTT6Bnz57w8vLCfffdB71eb7GNoqIiTJo0CZ6enggKCsLcuXPR2NjY1bticzt27MA999yD0NBQKBQKbNq0yWK9EAJZWVkICQmBh4cHtFotTp06ZdHm8uXLePDBB+Hj4wM/Pz88+uijqKqqsmhz6NAhjB07Fu7u7ggPD8fSpUs7e9ds7mbH6uGHH272OUtLS7No012OVXZ2NkaMGAFvb28EBQVhypQpKCgosGhjq9+7vLw8DB8+HGq1GjExMVi7dm1n755NtedY3XHHHc0+W7/97W8t2nSHY7V69WrExcVJA7+lpKTg888/l9Y7xGdKUIs2bNgg3NzcxDvvvCOOHj0qZs2aJfz8/IRer5e7tC61YMECMWjQIHHp0iXpVVJSIq3/7W9/K8LDw0Vubq7Yu3evGDlypBg1apS0vrGxUQwePFhotVpx4MABsXnzZhEYGCgyMzPl2B2b2rx5s3jxxRfFRx99JACIjz/+2GL9kiVLhK+vr9i0aZP47rvvxM9+9jMRFRUlamtrpTZpaWli6NChYteuXeKrr74SMTExYtq0adL6iooKodFoxIMPPiiOHDki1q9fLzw8PMRf/vKXrtpNm7jZsZoxY4ZIS0uz+JxdvnzZok13OVapqani3XffFUeOHBEHDx4Ud999t4iIiBBVVVVSG1v83n3//ffC09NTZGRkiGPHjom33npLqFQqkZOT06X7eyvac6xuv/12MWvWLIvPVkVFhbS+uxyr//znP+Kzzz4TJ0+eFAUFBeKFF14Qrq6u4siRI0IIx/hMMbC0IikpSTzxxBPSvMlkEqGhoSI7O1vGqrreggULxNChQ1tcV15eLlxdXcWHH34oLTt+/LgAIPLz84UQTV9USqVS6HQ6qc3q1auFj4+PqK+v79Tau9KPv4TNZrMIDg4Wr7/+urSsvLxcqNVqsX79eiGEEMeOHRMAxLfffiu1+fzzz4VCoRAXLlwQQgjx5z//Wfj7+1scq+eff17079+/k/eo87QWWCZPntzqe7rrsRJCiOLiYgFAbN++XQhhu9+75557TgwaNMjiZ6Wnp4vU1NTO3qVO8+NjJURTYHnqqadafU93PVZCCOHv7y/+9re/OcxnipeEWmA0GrFv3z5otVppmVKphFarRX5+voyVyePUqVMIDQ1FdHQ0HnzwQRQVFQEA9u3bh4aGBovjNGDAAEREREjHKT8/H0OGDIFGo5HapKamwmAw4OjRo127I12osLAQOp3O4tj4+voiOTnZ4tj4+fkhMTFRaqPVaqFUKrF7926pzbhx4+Dm5ia1SU1NRUFBAa5cudJFe9M18vLyEBQUhP79+2P27NkoKyuT1nXnY1VRUQEACAgIAGC737v8/HyLbVxr48h/4358rK755z//icDAQAwePBiZmZmoqamR1nXHY2UymbBhwwZUV1cjJSXFYT5TTvHwQ1srLS2FyWSy+B8DABqNBidOnJCpKnkkJydj7dq16N+/Py5duoRFixZh7NixOHLkCHQ6Hdzc3ODn52fxHo1GA51OBwDQ6XQtHsdr65zVtX1rad9vPDZBQUEW611cXBAQEGDRJioqqtk2rq3z9/fvlPq7WlpaGu69915ERUXhzJkzeOGFFzBx4kTk5+dDpVJ122NlNpvx9NNPY/To0Rg8eDAA2Oz3rrU2BoMBtbW18PDw6Ixd6jQtHSsAeOCBB9CnTx+Ehobi0KFDeP7551FQUICPPvoIQPc6VocPH0ZKSgrq6urg5eWFjz/+GAMHDsTBgwcd4jPFwEJtmjhxojQdFxeH5ORk9OnTBx988IHD/JKS/Zs6dao0PWTIEMTFxaFv377Iy8vD+PHjZaxMXk888QSOHDmCnTt3yl2K3WvtWD322GPS9JAhQxASEoLx48fjzJkz6Nu3b1eXKav+/fvj4MGDqKiowL/+9S/MmDED27dvl7usduMloRYEBgZCpVI16yGt1+sRHBwsU1X2wc/PD/369cPp06cRHBwMo9GI8vJyizY3Hqfg4OAWj+O1dc7q2r619RkKDg5GcXGxxfrGxkZcvny52x+/6OhoBAYG4vTp0wC657GaM2cOPv30U2zbtg29e/eWltvq9661Nj4+Pg73j5HWjlVLkpOTAcDis9VdjpWbmxtiYmKQkJCA7OxsDB06FH/6058c5jPFwNICNzc3JCQkIDc3V1pmNpuRm5uLlJQUGSuTX1VVFc6cOYOQkBAkJCTA1dXV4jgVFBSgqKhIOk4pKSk4fPiwxZfNli1b4OPjg4EDB3Z5/V0lKioKwcHBFsfGYDBg9+7dFsemvLwc+/btk9p8+eWXMJvN0h/VlJQU7NixAw0NDVKbLVu2oH///g55iaO9zp8/j7KyMoSEhADoXsdKCIE5c+bg448/xpdfftnsMpetfu9SUlIstnGtjSP9jbvZsWrJwYMHAcDis9UdjlVLzGYz6uvrHeczZZOuu05ow4YNQq1Wi7Vr14pjx46Jxx57TPj5+Vn0kO4OnnnmGZGXlycKCwvF119/LbRarQgMDBTFxcVCiKZb4SIiIsSXX34p9u7dK1JSUkRKSor0/mu3wk2YMEEcPHhQ5OTkiF69ejnFbc2VlZXiwIED4sCBAwKAWL58uThw4ID44YcfhBBNtzX7+fmJTz75RBw6dEhMnjy5xduahw0bJnbv3i127twpYmNjLW7VLS8vFxqNRjz00EPiyJEjYsOGDcLT09PhbtVt61hVVlaKZ599VuTn54vCwkKxdetWMXz4cBEbGyvq6uqkbXSXYzV79mzh6+sr8vLyLG7FrampkdrY4vfu2i2oc+fOFcePHxerVq1yuFt1b3asTp8+LV5++WWxd+9eUVhYKD755BMRHR0txo0bJ22juxyrefPmie3bt4vCwkJx6NAhMW/ePKFQKMT//vc/IYRjfKYYWNrw1ltviYiICOHm5iaSkpLErl275C6py6Wnp4uQkBDh5uYmwsLCRHp6ujh9+rS0vra2Vjz++OPC399feHp6ip///Ofi0qVLFts4e/asmDhxovDw8BCBgYHimWeeEQ0NDV29Kza3bds2AaDZa8aMGUKIplub58+fLzQajVCr1WL8+PGioKDAYhtlZWVi2rRpwsvLS/j4+IiZM2eKyspKizbfffedGDNmjFCr1SIsLEwsWbKkq3bRZto6VjU1NWLChAmiV69ewtXVVfTp00fMmjWr2T8Ousuxauk4ARDvvvuu1MZWv3fbtm0T8fHxws3NTURHR1v8DEdws2NVVFQkxo0bJwICAoRarRYxMTFi7ty5FuOwCNE9jtUjjzwi+vTpI9zc3ESvXr3E+PHjpbAihGN8phRCCGGbczVEREREnYN9WIiIiMjuMbAQERGR3WNgISIiIrvHwEJERER2j4GFiIiI7B4DCxEREdk9BhYiIiKyewwsREREZPcYWIiIiMjuMbAQERGR3WNgISIiIrvHwEJERER27/8BKIz6QA7AapsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualiser le retour attendu par episode.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, len(G)+1), G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pxki0G3HwMso"
   },
   "outputs": [],
   "source": [
    "# Extraire la politique optimale et executer là\n",
    "env = gym.make('FrozenLake-v1', desc=default_frozen_lake, is_slippery=False, render_mode=\"human\")\n",
    "politique_q_learning = extraire_politique_Qs(env, Qs)\n",
    "execution_politique(politique_q_learning, env)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
